# -*- coding: utf-8 -*-
"""api_basics_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/panaversity/learn-agentic-ai/blob/main/01_ai_agents_first/02_what_is_api/api_basics_demo.ipynb

# APIs — Hands‑on with Python
This Colab walks you through three live API calls:
1. Weather forecast
2. Cat facts
3. OpenAI Chat Completions

## 📦 installation
"""

!pip -q install requests google-genai

"""# 📦 Imports"""

import requests, pprint

"""##  ⚙️ 1. Get current weather for London (Simple api call )"""

url = "https://api.open-meteo.com/v1/forecast?latitude=51.5&longitude=-0.12&current_weather=true"
resp = requests.get(url, timeout=10)
resp.raise_for_status()
weather = resp.json()
pprint.pp(weather)

"""## ⚙️ 2. Swap endpoint: Cat Fact"""

url = "https://catfact.ninja/fact"
resp = requests.get(url, timeout=10)
resp.raise_for_status()
print(resp.json()['fact'])

"""## 3. 🔐 Load API Key
Fill in your API key below. You can store it in an environment variable or directly in the code (not recommended for production).
"""

from google.colab import userdata
import google.generativeai as genai

GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')

"""## Checking Behavior - Stateless or Stateful"""

import os
import google.generativeai as genai
from google.colab import userdata

# 🔑 TODO: Replace with your own key or set OPENAI_API_KEY in the environment
gemini_api_key = userdata.get('GEMINI_API_KEY')

genai.configure(api_key=gemini_api_key)

model = genai.GenerativeModel('gemini-2.5-flash') # Using a preview version

response = model.generate_content("Say hello! I'm Wania")

print(response.text)

response = genai.GenerativeModel("gemini-2.5-flash")
response = response.generate_content("what's my name?")
print(response.text)

"""# Responses API

## Checking Behavior - Stateless or Stateful
"""

import google.generativeai as genai
from google.colab import userdata

GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')

genai.configure(api_key=GEMINI_API_KEY)

model = genai.GenerativeModel('gemini-2.5-flash') # Use a model that supports multimodal input

response = model.generate_content([
    "what is in this image?",
    {
        "mime_type": "image/jpeg",
        "data": requests.get("https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg").content
    }
])

print(response.text)

import google.generativeai as genai
from google.colab import userdata

GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')

client = genai.configure(api_key=GEMINI_API_KEY)

model = genai.GenerativeModel('gemini-2.5-flash')

response = model.generate_content("hello, I'm Wania")

print(response.text)

next_response = client.responses.create(
    model="gpt-4.1",
    store=True,
    previous_response_id=response.id,  # 👈 linking to earlier message
    input=[
        {
            "role": "user",
            "content": [
                { "type": "input_text", "text": "What is my name?" },
            ]
        }
    ]
)

print("Follow-up response:", next_response.output[0].content[0].text)

"""# 😊 Gemini Code

# 📦 Imports
"""

from openai import OpenAI
from google.colab import userdata

"""# 🔐 Load API Key"""

api_key = userdata.get('GEMINI_API_KEY')

"""# 🤖 Initialize Gemini Client"""

client = OpenAI(
    api_key=api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

"""# 💬 Run Basic Chat Completion"""

def main():
    print("🧠 Asking Gemini a question...\n")

    response = client.chat.completions.create(
        model="gemini-2.5-flash",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user",   "content": "Explain how AI works in simple terms."}
        ]
    )

    message = response.choices[0].message.content
    print("💡 Gemini's Response:\n")
    print(message)

"""# 🚀 Entry Point"""

if __name__ == "__main__":
    main()

for m in genai.list_models():
  if 'generateContent' in m.supported_generation_methods:
    print(m.name)

import google.generativeai as genai
from google.colab import userdata

GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')

genai.configure(api_key=GEMINI_API_KEY)

model = genai.GenerativeModel('gemini-2.5-flash')

# Start a new chat session
chat = model.start_chat(history=[])

# Send the first message
response1 = chat.send_message("hello, I'm Wania")
print("First response:", response1.text)

# Now, in a separate cell or later in this cell, you can send a follow-up message using the same 'chat' object
# For example, in a new cell, you would use:
# response2 = chat.send_message("What is my name?")
# print("Second response:", response2.text)